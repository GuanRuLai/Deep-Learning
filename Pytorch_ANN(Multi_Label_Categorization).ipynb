{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfibZ3dVzeV0FMvdPPGQ4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuanRuLai/Python-Deep-Learning/blob/main/Pytorch_ANN(Multi_Label_Categorization).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "plzTQYTrxI5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library"
      ],
      "metadata": {
        "id": "0QU1qQw8sLc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxwQcOZlqdJX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isdir(\"HappyML\"):\n",
        "  os.system(\"git clone https://github.com/cnchi/HappyML.git\")\n",
        "\n",
        "import HappyML.preprocessor as pp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if there is GPU to use"
      ],
      "metadata": {
        "id": "UDHocrmhtdHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU available, using CPU instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA24vfQ8teaL",
        "outputId": "4a7fce5c-67f1-42c6-c1cc-b05e1defe178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset & Split independent variables and dependent variable"
      ],
      "metadata": {
        "id": "Qsv-zGkat5Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_iris()\n",
        "X = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "y = pd.DataFrame(dataset.target, columns=[\"Iris_type\"])\n",
        "print(X.head())\n",
        "print(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrQCDAJ3t8sv",
        "outputId": "20f5c59a-34aa-4b53-d282-93f38d91c4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "   Iris_type\n",
            "0          0\n",
            "1          0\n",
            "2          0\n",
            "3          0\n",
            "4          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split training set and testing set"
      ],
      "metadata": {
        "id": "ooiMpSIEumga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "print(Y_train)\n",
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xk0gcIDuoZ1",
        "outputId": "9fd3ec89-ed45-489d-9f1b-67e319e7189b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "137                6.4               3.1                5.5               1.8\n",
            "84                 5.4               3.0                4.5               1.5\n",
            "27                 5.2               3.5                1.5               0.2\n",
            "127                6.1               3.0                4.9               1.8\n",
            "132                6.4               2.8                5.6               2.2\n",
            "..                 ...               ...                ...               ...\n",
            "9                  4.9               3.1                1.5               0.1\n",
            "103                6.3               2.9                5.6               1.8\n",
            "67                 5.8               2.7                4.1               1.0\n",
            "117                7.7               3.8                6.7               2.2\n",
            "47                 4.6               3.2                1.4               0.2\n",
            "\n",
            "[120 rows x 4 columns]\n",
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "114                5.8               2.8                5.1               2.4\n",
            "62                 6.0               2.2                4.0               1.0\n",
            "33                 5.5               4.2                1.4               0.2\n",
            "107                7.3               2.9                6.3               1.8\n",
            "7                  5.0               3.4                1.5               0.2\n",
            "100                6.3               3.3                6.0               2.5\n",
            "40                 5.0               3.5                1.3               0.3\n",
            "86                 6.7               3.1                4.7               1.5\n",
            "76                 6.8               2.8                4.8               1.4\n",
            "71                 6.1               2.8                4.0               1.3\n",
            "134                6.1               2.6                5.6               1.4\n",
            "51                 6.4               3.2                4.5               1.5\n",
            "73                 6.1               2.8                4.7               1.2\n",
            "54                 6.5               2.8                4.6               1.5\n",
            "63                 6.1               2.9                4.7               1.4\n",
            "37                 4.9               3.6                1.4               0.1\n",
            "78                 6.0               2.9                4.5               1.5\n",
            "90                 5.5               2.6                4.4               1.2\n",
            "45                 4.8               3.0                1.4               0.3\n",
            "16                 5.4               3.9                1.3               0.4\n",
            "121                5.6               2.8                4.9               2.0\n",
            "66                 5.6               3.0                4.5               1.5\n",
            "24                 4.8               3.4                1.9               0.2\n",
            "8                  4.4               2.9                1.4               0.2\n",
            "126                6.2               2.8                4.8               1.8\n",
            "22                 4.6               3.6                1.0               0.2\n",
            "44                 5.1               3.8                1.9               0.4\n",
            "97                 6.2               2.9                4.3               1.3\n",
            "93                 5.0               2.3                3.3               1.0\n",
            "26                 5.0               3.4                1.6               0.4\n",
            "     Iris_type\n",
            "137          2\n",
            "84           1\n",
            "27           0\n",
            "127          2\n",
            "132          2\n",
            "..         ...\n",
            "9            0\n",
            "103          2\n",
            "67           1\n",
            "117          2\n",
            "47           0\n",
            "\n",
            "[120 rows x 1 columns]\n",
            "     Iris_type\n",
            "114          2\n",
            "62           1\n",
            "33           0\n",
            "107          2\n",
            "7            0\n",
            "100          2\n",
            "40           0\n",
            "86           1\n",
            "76           1\n",
            "71           1\n",
            "134          2\n",
            "51           1\n",
            "73           1\n",
            "54           1\n",
            "63           1\n",
            "37           0\n",
            "78           1\n",
            "90           1\n",
            "45           0\n",
            "16           0\n",
            "121          2\n",
            "66           1\n",
            "24           0\n",
            "8            0\n",
            "126          2\n",
            "22           0\n",
            "44           0\n",
            "97           1\n",
            "93           1\n",
            "26           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature scaling"
      ],
      "metadata": {
        "id": "ntNyd1MMvDYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "print(X_train)\n",
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaZztEBbvFEj",
        "outputId": "59fb9dfa-3995-4dc5-dab1-4b14f8cadc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.61303014  0.10850105  0.94751783  0.736072  ]\n",
            " [-0.56776627 -0.12400121  0.38491447  0.34752959]\n",
            " [-0.80392556  1.03851009 -1.30289562 -1.33615415]\n",
            " [ 0.25879121 -0.12400121  0.60995581  0.736072  ]\n",
            " [ 0.61303014 -0.58900572  1.00377816  1.25412853]\n",
            " [-0.80392556 -0.82150798  0.04735245  0.21801546]\n",
            " [-0.21352735  1.73601687 -1.19037495 -1.20664002]\n",
            " [ 0.14071157 -0.82150798  0.72247648  0.47704373]\n",
            " [ 0.02263193 -0.12400121  0.21613346  0.34752959]\n",
            " [-0.09544771 -1.05401024  0.10361279 -0.04101281]\n",
            " [ 1.0853487  -0.12400121  0.94751783  1.1246144 ]\n",
            " [-1.39432376  0.34100331 -1.41541629 -1.33615415]\n",
            " [ 1.20342834  0.10850105  0.72247648  1.38364267]\n",
            " [-1.04008484  1.03851009 -1.24663528 -0.81809761]\n",
            " [-0.56776627  1.50351461 -1.30289562 -1.33615415]\n",
            " [-1.04008484 -2.4490238  -0.1776889  -0.30004108]\n",
            " [ 0.73110978 -0.12400121  0.94751783  0.736072  ]\n",
            " [ 0.96726906  0.57350557  1.0600385   1.64267094]\n",
            " [ 0.14071157 -1.98401928  0.66621615  0.34752959]\n",
            " [ 0.96726906 -1.2865125   1.11629884  0.736072  ]\n",
            " [-0.33160699 -1.2865125   0.04735245 -0.17052694]\n",
            " [ 2.14806547 -0.12400121  1.28507985  1.38364267]\n",
            " [ 0.49495049  0.57350557  0.49743514  0.47704373]\n",
            " [-0.44968663 -1.51901476 -0.00890789 -0.17052694]\n",
            " [ 0.49495049 -0.82150798  0.60995581  0.736072  ]\n",
            " [ 0.49495049 -0.58900572  0.72247648  0.34752959]\n",
            " [-1.15816448 -1.2865125   0.38491447  0.60655786]\n",
            " [ 0.49495049 -1.2865125   0.66621615  0.86558613]\n",
            " [ 1.32150798  0.34100331  0.49743514  0.21801546]\n",
            " [ 0.73110978 -0.12400121  0.77873682  0.99510027]\n",
            " [ 0.14071157  0.80600783  0.38491447  0.47704373]\n",
            " [-1.27624412  0.10850105 -1.24663528 -1.33615415]\n",
            " [-0.09544771 -0.82150798  0.72247648  0.86558613]\n",
            " [-0.33160699 -0.82150798  0.21613346  0.08850133]\n",
            " [-0.33160699 -0.35650346 -0.12142856  0.08850133]\n",
            " [-0.44968663 -1.2865125   0.10361279  0.08850133]\n",
            " [ 0.25879121 -0.12400121  0.4411748   0.21801546]\n",
            " [ 1.55766726  0.34100331  1.22881951  0.736072  ]\n",
            " [-0.68584591  1.50351461 -1.30289562 -1.33615415]\n",
            " [-1.86664232 -0.12400121 -1.52793696 -1.46566829]\n",
            " [ 0.61303014 -0.82150798  0.83499716  0.86558613]\n",
            " [-0.21352735 -0.12400121  0.21613346 -0.04101281]\n",
            " [-0.56776627  0.80600783 -1.19037495 -1.33615415]\n",
            " [-0.21352735  3.13103043 -1.30289562 -1.07712588]\n",
            " [ 1.20342834  0.10850105  0.60995581  0.34752959]\n",
            " [-1.5124034   0.10850105 -1.30289562 -1.33615415]\n",
            " [ 0.02263193 -0.12400121  0.72247648  0.736072  ]\n",
            " [-0.9220052  -1.2865125  -0.45899058 -0.17052694]\n",
            " [-1.5124034   0.80600783 -1.35915595 -1.20664002]\n",
            " [ 0.37687085 -1.98401928  0.38491447  0.34752959]\n",
            " [ 1.55766726  1.27101235  1.28507985  1.64267094]\n",
            " [-0.21352735 -0.35650346  0.21613346  0.08850133]\n",
            " [-1.27624412 -0.12400121 -1.35915595 -1.46566829]\n",
            " [ 1.43958762 -0.12400121  1.17255917  1.1246144 ]\n",
            " [ 1.20342834  0.34100331  1.0600385   1.38364267]\n",
            " [ 0.73110978 -0.12400121  1.11629884  1.25412853]\n",
            " [ 0.61303014 -0.58900572  1.00377816  1.1246144 ]\n",
            " [-0.9220052   1.73601687 -1.24663528 -1.33615415]\n",
            " [-1.27624412  0.80600783 -1.24663528 -1.33615415]\n",
            " [ 0.73110978  0.34100331  0.72247648  0.99510027]\n",
            " [ 0.96726906  0.57350557  1.0600385   1.1246144 ]\n",
            " [-1.63048304 -1.75151702 -1.41541629 -1.20664002]\n",
            " [ 0.37687085  0.80600783  0.89125749  1.38364267]\n",
            " [-1.15816448 -0.12400121 -1.35915595 -1.33615415]\n",
            " [-0.21352735 -1.2865125   0.66621615  0.99510027]\n",
            " [ 1.20342834  0.10850105  0.89125749  1.1246144 ]\n",
            " [-1.74856268  0.34100331 -1.41541629 -1.33615415]\n",
            " [-1.04008484  1.27101235 -1.35915595 -1.33615415]\n",
            " [ 1.55766726 -0.12400121  1.11629884  0.47704373]\n",
            " [-0.9220052   1.03851009 -1.35915595 -1.20664002]\n",
            " [-1.74856268 -0.12400121 -1.41541629 -1.33615415]\n",
            " [-0.56776627  1.96851913 -1.19037495 -1.07712588]\n",
            " [-0.44968663 -1.75151702  0.10361279  0.08850133]\n",
            " [ 1.0853487   0.34100331  1.17255917  1.38364267]\n",
            " [ 2.02998583 -0.12400121  1.56638153  1.1246144 ]\n",
            " [-0.9220052   1.03851009 -1.35915595 -1.33615415]\n",
            " [-1.15816448  0.10850105 -1.30289562 -1.33615415]\n",
            " [-0.80392556  0.80600783 -1.35915595 -1.33615415]\n",
            " [-0.21352735 -0.58900572  0.38491447  0.08850133]\n",
            " [ 0.84918942 -0.12400121  0.32865413  0.21801546]\n",
            " [-1.04008484  0.34100331 -1.47167663 -1.33615415]\n",
            " [-0.9220052   0.57350557 -1.19037495 -0.94761175]\n",
            " [ 0.61303014 -0.35650346  0.27239379  0.08850133]\n",
            " [-0.56776627  0.80600783 -1.30289562 -1.07712588]\n",
            " [ 2.14806547 -1.05401024  1.73516253  1.38364267]\n",
            " [-1.15816448 -1.51901476 -0.29020957 -0.30004108]\n",
            " [ 2.38422475  1.73601687  1.45386085  0.99510027]\n",
            " [ 0.96726906  0.10850105  0.32865413  0.21801546]\n",
            " [-0.80392556  2.43352365 -1.30289562 -1.46566829]\n",
            " [ 0.14071157 -0.12400121  0.55369548  0.736072  ]\n",
            " [-0.09544771  2.20102139 -1.47167663 -1.33615415]\n",
            " [ 2.14806547 -0.58900572  1.62264186  0.99510027]\n",
            " [-0.9220052   1.73601687 -1.30289562 -1.20664002]\n",
            " [-1.39432376  0.34100331 -1.24663528 -1.33615415]\n",
            " [ 1.79382654 -0.58900572  1.28507985  0.86558613]\n",
            " [-1.04008484  0.57350557 -1.35915595 -1.33615415]\n",
            " [ 0.49495049  0.80600783  1.00377816  1.5131568 ]\n",
            " [-0.21352735 -0.58900572  0.15987312  0.08850133]\n",
            " [-0.09544771 -0.82150798  0.04735245 -0.04101281]\n",
            " [-0.21352735 -1.05401024 -0.1776889  -0.30004108]\n",
            " [ 0.61303014  0.34100331  0.83499716  1.38364267]\n",
            " [ 0.96726906 -0.12400121  0.77873682  1.38364267]\n",
            " [ 0.49495049 -1.2865125   0.60995581  0.34752959]\n",
            " [ 0.96726906 -0.12400121  0.66621615  0.60655786]\n",
            " [-1.04008484 -0.12400121 -1.24663528 -1.33615415]\n",
            " [-0.44968663 -1.51901476 -0.06516822 -0.30004108]\n",
            " [ 0.96726906  0.10850105  1.00377816  1.5131568 ]\n",
            " [-0.09544771 -0.82150798  0.72247648  0.86558613]\n",
            " [-0.9220052   0.80600783 -1.30289562 -1.33615415]\n",
            " [ 0.84918942 -0.35650346  0.4411748   0.08850133]\n",
            " [-0.33160699 -0.12400121  0.15987312  0.08850133]\n",
            " [ 0.02263193  0.34100331  0.55369548  0.736072  ]\n",
            " [ 0.49495049 -1.75151702  0.32865413  0.08850133]\n",
            " [-0.44968663  1.03851009 -1.41541629 -1.33615415]\n",
            " [-0.9220052   1.50351461 -1.30289562 -1.07712588]\n",
            " [-1.15816448  0.10850105 -1.30289562 -1.46566829]\n",
            " [ 0.49495049 -0.35650346  1.00377816  0.736072  ]\n",
            " [-0.09544771 -0.82150798  0.15987312 -0.30004108]\n",
            " [ 2.14806547  1.73601687  1.62264186  1.25412853]\n",
            " [-1.5124034   0.34100331 -1.35915595 -1.33615415]]\n",
            "[[-0.09544771 -0.58900572  0.72247648  1.5131568 ]\n",
            " [ 0.14071157 -1.98401928  0.10361279 -0.30004108]\n",
            " [-0.44968663  2.66602591 -1.35915595 -1.33615415]\n",
            " [ 1.6757469  -0.35650346  1.39760052  0.736072  ]\n",
            " [-1.04008484  0.80600783 -1.30289562 -1.33615415]\n",
            " [ 0.49495049  0.57350557  1.22881951  1.64267094]\n",
            " [-1.04008484  1.03851009 -1.41541629 -1.20664002]\n",
            " [ 0.96726906  0.10850105  0.49743514  0.34752959]\n",
            " [ 1.0853487  -0.58900572  0.55369548  0.21801546]\n",
            " [ 0.25879121 -0.58900572  0.10361279  0.08850133]\n",
            " [ 0.25879121 -1.05401024  1.00377816  0.21801546]\n",
            " [ 0.61303014  0.34100331  0.38491447  0.34752959]\n",
            " [ 0.25879121 -0.58900572  0.49743514 -0.04101281]\n",
            " [ 0.73110978 -0.58900572  0.4411748   0.34752959]\n",
            " [ 0.25879121 -0.35650346  0.49743514  0.21801546]\n",
            " [-1.15816448  1.27101235 -1.35915595 -1.46566829]\n",
            " [ 0.14071157 -0.35650346  0.38491447  0.34752959]\n",
            " [-0.44968663 -1.05401024  0.32865413 -0.04101281]\n",
            " [-1.27624412 -0.12400121 -1.35915595 -1.20664002]\n",
            " [-0.56776627  1.96851913 -1.41541629 -1.07712588]\n",
            " [-0.33160699 -0.58900572  0.60995581  0.99510027]\n",
            " [-0.33160699 -0.12400121  0.38491447  0.34752959]\n",
            " [-1.27624412  0.80600783 -1.07785427 -1.33615415]\n",
            " [-1.74856268 -0.35650346 -1.35915595 -1.33615415]\n",
            " [ 0.37687085 -0.58900572  0.55369548  0.736072  ]\n",
            " [-1.5124034   1.27101235 -1.5841973  -1.33615415]\n",
            " [-0.9220052   1.73601687 -1.07785427 -1.07712588]\n",
            " [ 0.37687085 -0.35650346  0.27239379  0.08850133]\n",
            " [-1.04008484 -1.75151702 -0.29020957 -0.30004108]\n",
            " [-1.04008484  0.80600783 -1.24663528 -1.07712588]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert data into tensor"
      ],
      "metadata": {
        "id": "DS4x7BtBv1F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
        "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
        "print(X_train_tensor)\n",
        "print(X_test_tensor)\n",
        "print(Y_train_tensor)\n",
        "print(Y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_tAXz7nv5nN",
        "outputId": "0d32c43d-4dc0-4640-894e-58a5edfaf2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6130,  0.1085,  0.9475,  0.7361],\n",
            "        [-0.5678, -0.1240,  0.3849,  0.3475],\n",
            "        [-0.8039,  1.0385, -1.3029, -1.3362],\n",
            "        [ 0.2588, -0.1240,  0.6100,  0.7361],\n",
            "        [ 0.6130, -0.5890,  1.0038,  1.2541],\n",
            "        [-0.8039, -0.8215,  0.0474,  0.2180],\n",
            "        [-0.2135,  1.7360, -1.1904, -1.2066],\n",
            "        [ 0.1407, -0.8215,  0.7225,  0.4770],\n",
            "        [ 0.0226, -0.1240,  0.2161,  0.3475],\n",
            "        [-0.0954, -1.0540,  0.1036, -0.0410],\n",
            "        [ 1.0853, -0.1240,  0.9475,  1.1246],\n",
            "        [-1.3943,  0.3410, -1.4154, -1.3362],\n",
            "        [ 1.2034,  0.1085,  0.7225,  1.3836],\n",
            "        [-1.0401,  1.0385, -1.2466, -0.8181],\n",
            "        [-0.5678,  1.5035, -1.3029, -1.3362],\n",
            "        [-1.0401, -2.4490, -0.1777, -0.3000],\n",
            "        [ 0.7311, -0.1240,  0.9475,  0.7361],\n",
            "        [ 0.9673,  0.5735,  1.0600,  1.6427],\n",
            "        [ 0.1407, -1.9840,  0.6662,  0.3475],\n",
            "        [ 0.9673, -1.2865,  1.1163,  0.7361],\n",
            "        [-0.3316, -1.2865,  0.0474, -0.1705],\n",
            "        [ 2.1481, -0.1240,  1.2851,  1.3836],\n",
            "        [ 0.4950,  0.5735,  0.4974,  0.4770],\n",
            "        [-0.4497, -1.5190, -0.0089, -0.1705],\n",
            "        [ 0.4950, -0.8215,  0.6100,  0.7361],\n",
            "        [ 0.4950, -0.5890,  0.7225,  0.3475],\n",
            "        [-1.1582, -1.2865,  0.3849,  0.6066],\n",
            "        [ 0.4950, -1.2865,  0.6662,  0.8656],\n",
            "        [ 1.3215,  0.3410,  0.4974,  0.2180],\n",
            "        [ 0.7311, -0.1240,  0.7787,  0.9951],\n",
            "        [ 0.1407,  0.8060,  0.3849,  0.4770],\n",
            "        [-1.2762,  0.1085, -1.2466, -1.3362],\n",
            "        [-0.0954, -0.8215,  0.7225,  0.8656],\n",
            "        [-0.3316, -0.8215,  0.2161,  0.0885],\n",
            "        [-0.3316, -0.3565, -0.1214,  0.0885],\n",
            "        [-0.4497, -1.2865,  0.1036,  0.0885],\n",
            "        [ 0.2588, -0.1240,  0.4412,  0.2180],\n",
            "        [ 1.5577,  0.3410,  1.2288,  0.7361],\n",
            "        [-0.6858,  1.5035, -1.3029, -1.3362],\n",
            "        [-1.8666, -0.1240, -1.5279, -1.4657],\n",
            "        [ 0.6130, -0.8215,  0.8350,  0.8656],\n",
            "        [-0.2135, -0.1240,  0.2161, -0.0410],\n",
            "        [-0.5678,  0.8060, -1.1904, -1.3362],\n",
            "        [-0.2135,  3.1310, -1.3029, -1.0771],\n",
            "        [ 1.2034,  0.1085,  0.6100,  0.3475],\n",
            "        [-1.5124,  0.1085, -1.3029, -1.3362],\n",
            "        [ 0.0226, -0.1240,  0.7225,  0.7361],\n",
            "        [-0.9220, -1.2865, -0.4590, -0.1705],\n",
            "        [-1.5124,  0.8060, -1.3592, -1.2066],\n",
            "        [ 0.3769, -1.9840,  0.3849,  0.3475],\n",
            "        [ 1.5577,  1.2710,  1.2851,  1.6427],\n",
            "        [-0.2135, -0.3565,  0.2161,  0.0885],\n",
            "        [-1.2762, -0.1240, -1.3592, -1.4657],\n",
            "        [ 1.4396, -0.1240,  1.1726,  1.1246],\n",
            "        [ 1.2034,  0.3410,  1.0600,  1.3836],\n",
            "        [ 0.7311, -0.1240,  1.1163,  1.2541],\n",
            "        [ 0.6130, -0.5890,  1.0038,  1.1246],\n",
            "        [-0.9220,  1.7360, -1.2466, -1.3362],\n",
            "        [-1.2762,  0.8060, -1.2466, -1.3362],\n",
            "        [ 0.7311,  0.3410,  0.7225,  0.9951],\n",
            "        [ 0.9673,  0.5735,  1.0600,  1.1246],\n",
            "        [-1.6305, -1.7515, -1.4154, -1.2066],\n",
            "        [ 0.3769,  0.8060,  0.8913,  1.3836],\n",
            "        [-1.1582, -0.1240, -1.3592, -1.3362],\n",
            "        [-0.2135, -1.2865,  0.6662,  0.9951],\n",
            "        [ 1.2034,  0.1085,  0.8913,  1.1246],\n",
            "        [-1.7486,  0.3410, -1.4154, -1.3362],\n",
            "        [-1.0401,  1.2710, -1.3592, -1.3362],\n",
            "        [ 1.5577, -0.1240,  1.1163,  0.4770],\n",
            "        [-0.9220,  1.0385, -1.3592, -1.2066],\n",
            "        [-1.7486, -0.1240, -1.4154, -1.3362],\n",
            "        [-0.5678,  1.9685, -1.1904, -1.0771],\n",
            "        [-0.4497, -1.7515,  0.1036,  0.0885],\n",
            "        [ 1.0853,  0.3410,  1.1726,  1.3836],\n",
            "        [ 2.0300, -0.1240,  1.5664,  1.1246],\n",
            "        [-0.9220,  1.0385, -1.3592, -1.3362],\n",
            "        [-1.1582,  0.1085, -1.3029, -1.3362],\n",
            "        [-0.8039,  0.8060, -1.3592, -1.3362],\n",
            "        [-0.2135, -0.5890,  0.3849,  0.0885],\n",
            "        [ 0.8492, -0.1240,  0.3287,  0.2180],\n",
            "        [-1.0401,  0.3410, -1.4717, -1.3362],\n",
            "        [-0.9220,  0.5735, -1.1904, -0.9476],\n",
            "        [ 0.6130, -0.3565,  0.2724,  0.0885],\n",
            "        [-0.5678,  0.8060, -1.3029, -1.0771],\n",
            "        [ 2.1481, -1.0540,  1.7352,  1.3836],\n",
            "        [-1.1582, -1.5190, -0.2902, -0.3000],\n",
            "        [ 2.3842,  1.7360,  1.4539,  0.9951],\n",
            "        [ 0.9673,  0.1085,  0.3287,  0.2180],\n",
            "        [-0.8039,  2.4335, -1.3029, -1.4657],\n",
            "        [ 0.1407, -0.1240,  0.5537,  0.7361],\n",
            "        [-0.0954,  2.2010, -1.4717, -1.3362],\n",
            "        [ 2.1481, -0.5890,  1.6226,  0.9951],\n",
            "        [-0.9220,  1.7360, -1.3029, -1.2066],\n",
            "        [-1.3943,  0.3410, -1.2466, -1.3362],\n",
            "        [ 1.7938, -0.5890,  1.2851,  0.8656],\n",
            "        [-1.0401,  0.5735, -1.3592, -1.3362],\n",
            "        [ 0.4950,  0.8060,  1.0038,  1.5132],\n",
            "        [-0.2135, -0.5890,  0.1599,  0.0885],\n",
            "        [-0.0954, -0.8215,  0.0474, -0.0410],\n",
            "        [-0.2135, -1.0540, -0.1777, -0.3000],\n",
            "        [ 0.6130,  0.3410,  0.8350,  1.3836],\n",
            "        [ 0.9673, -0.1240,  0.7787,  1.3836],\n",
            "        [ 0.4950, -1.2865,  0.6100,  0.3475],\n",
            "        [ 0.9673, -0.1240,  0.6662,  0.6066],\n",
            "        [-1.0401, -0.1240, -1.2466, -1.3362],\n",
            "        [-0.4497, -1.5190, -0.0652, -0.3000],\n",
            "        [ 0.9673,  0.1085,  1.0038,  1.5132],\n",
            "        [-0.0954, -0.8215,  0.7225,  0.8656],\n",
            "        [-0.9220,  0.8060, -1.3029, -1.3362],\n",
            "        [ 0.8492, -0.3565,  0.4412,  0.0885],\n",
            "        [-0.3316, -0.1240,  0.1599,  0.0885],\n",
            "        [ 0.0226,  0.3410,  0.5537,  0.7361],\n",
            "        [ 0.4950, -1.7515,  0.3287,  0.0885],\n",
            "        [-0.4497,  1.0385, -1.4154, -1.3362],\n",
            "        [-0.9220,  1.5035, -1.3029, -1.0771],\n",
            "        [-1.1582,  0.1085, -1.3029, -1.4657],\n",
            "        [ 0.4950, -0.3565,  1.0038,  0.7361],\n",
            "        [-0.0954, -0.8215,  0.1599, -0.3000],\n",
            "        [ 2.1481,  1.7360,  1.6226,  1.2541],\n",
            "        [-1.5124,  0.3410, -1.3592, -1.3362]])\n",
            "tensor([[-0.0954, -0.5890,  0.7225,  1.5132],\n",
            "        [ 0.1407, -1.9840,  0.1036, -0.3000],\n",
            "        [-0.4497,  2.6660, -1.3592, -1.3362],\n",
            "        [ 1.6757, -0.3565,  1.3976,  0.7361],\n",
            "        [-1.0401,  0.8060, -1.3029, -1.3362],\n",
            "        [ 0.4950,  0.5735,  1.2288,  1.6427],\n",
            "        [-1.0401,  1.0385, -1.4154, -1.2066],\n",
            "        [ 0.9673,  0.1085,  0.4974,  0.3475],\n",
            "        [ 1.0853, -0.5890,  0.5537,  0.2180],\n",
            "        [ 0.2588, -0.5890,  0.1036,  0.0885],\n",
            "        [ 0.2588, -1.0540,  1.0038,  0.2180],\n",
            "        [ 0.6130,  0.3410,  0.3849,  0.3475],\n",
            "        [ 0.2588, -0.5890,  0.4974, -0.0410],\n",
            "        [ 0.7311, -0.5890,  0.4412,  0.3475],\n",
            "        [ 0.2588, -0.3565,  0.4974,  0.2180],\n",
            "        [-1.1582,  1.2710, -1.3592, -1.4657],\n",
            "        [ 0.1407, -0.3565,  0.3849,  0.3475],\n",
            "        [-0.4497, -1.0540,  0.3287, -0.0410],\n",
            "        [-1.2762, -0.1240, -1.3592, -1.2066],\n",
            "        [-0.5678,  1.9685, -1.4154, -1.0771],\n",
            "        [-0.3316, -0.5890,  0.6100,  0.9951],\n",
            "        [-0.3316, -0.1240,  0.3849,  0.3475],\n",
            "        [-1.2762,  0.8060, -1.0779, -1.3362],\n",
            "        [-1.7486, -0.3565, -1.3592, -1.3362],\n",
            "        [ 0.3769, -0.5890,  0.5537,  0.7361],\n",
            "        [-1.5124,  1.2710, -1.5842, -1.3362],\n",
            "        [-0.9220,  1.7360, -1.0779, -1.0771],\n",
            "        [ 0.3769, -0.3565,  0.2724,  0.0885],\n",
            "        [-1.0401, -1.7515, -0.2902, -0.3000],\n",
            "        [-1.0401,  0.8060, -1.2466, -1.0771]])\n",
            "tensor([[2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [1],\n",
            "        [2],\n",
            "        [1],\n",
            "        [1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [1],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [2],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [2],\n",
            "        [0]])\n",
            "tensor([[2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [2],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [1],\n",
            "        [0],\n",
            "        [0],\n",
            "        [2],\n",
            "        [0],\n",
            "        [0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network processing"
      ],
      "metadata": {
        "id": "g_eL4nuWxUTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "cYwuTBIcxAkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IrisModel(nn.Module):\n",
        "\n",
        "  # define frameworks of each neural layer\n",
        "  def __init__(self):\n",
        "    super(IrisModel, self).__init__()\n",
        "\n",
        "    # define neural layers\n",
        "    self.fc1 = nn.Linear(4, 16)\n",
        "    self.fc2 = nn.Linear(16, 16)\n",
        "    self.fc3 = nn.Linear(16, 3)\n",
        "\n",
        "    # define weight initializers of each layer(default)\n",
        "    init.xavier_normal_(self.fc1.weight)\n",
        "    init.xavier_normal_(self.fc2.weight)\n",
        "    init.xavier_normal_(self.fc3.weight)\n",
        "\n",
        "  # define forward propagation function to connect layers(including activation function)\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# initialize model\n",
        "model = IrisModel().to(device)\n",
        "\n",
        "# define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "a4LOBwkuxFHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training & evaluation"
      ],
      "metadata": {
        "id": "FfDdfLaN0C-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
        "test_dataset =  TensorDataset(X_test_tensor, Y_test_tensor)\n",
        "# cut dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "-0sMUoi30GKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  model.train() # weights can be modified\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "    optimizer.zero_grad() # return zero of every previous batch\n",
        "\n",
        "    Y_pred = model(X_batch)\n",
        "    loss = criterion(Y_pred, Y_batch.squeeze())\n",
        "    loss.backward() # calculate gradient(min loss weights)\n",
        "    optimizer.step() # update weights\n",
        "\n",
        "    _, predicted = torch.max(Y_pred.data, 1) # get the index of max value in each row of axis 1\n",
        "    total += Y_batch.size(0) # get the number of samples\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "  accuracy = correct / total\n",
        "  print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAXHrFB23BcA",
        "outputId": "b7405412-234d-41c6-bb08-49c5d4b52027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100, Loss: 1.1860, Acc: 0.1667\n",
            "Epoch: 2/100, Loss: 1.1813, Acc: 0.2083\n",
            "Epoch: 3/100, Loss: 1.1581, Acc: 0.2500\n",
            "Epoch: 4/100, Loss: 1.0811, Acc: 0.2917\n",
            "Epoch: 5/100, Loss: 1.0382, Acc: 0.3833\n",
            "Epoch: 6/100, Loss: 1.0784, Acc: 0.4583\n",
            "Epoch: 7/100, Loss: 1.0624, Acc: 0.4917\n",
            "Epoch: 8/100, Loss: 1.0318, Acc: 0.5417\n",
            "Epoch: 9/100, Loss: 0.9995, Acc: 0.5583\n",
            "Epoch: 10/100, Loss: 0.9639, Acc: 0.5917\n",
            "Epoch: 11/100, Loss: 0.9280, Acc: 0.6167\n",
            "Epoch: 12/100, Loss: 1.0118, Acc: 0.6167\n",
            "Epoch: 13/100, Loss: 0.9587, Acc: 0.6500\n",
            "Epoch: 14/100, Loss: 0.9496, Acc: 0.6583\n",
            "Epoch: 15/100, Loss: 0.9514, Acc: 0.6750\n",
            "Epoch: 16/100, Loss: 0.9159, Acc: 0.6750\n",
            "Epoch: 17/100, Loss: 0.8972, Acc: 0.6833\n",
            "Epoch: 18/100, Loss: 0.8833, Acc: 0.6833\n",
            "Epoch: 19/100, Loss: 0.8680, Acc: 0.6917\n",
            "Epoch: 20/100, Loss: 0.8823, Acc: 0.6917\n",
            "Epoch: 21/100, Loss: 0.9314, Acc: 0.7000\n",
            "Epoch: 22/100, Loss: 0.9045, Acc: 0.7000\n",
            "Epoch: 23/100, Loss: 0.6554, Acc: 0.7083\n",
            "Epoch: 24/100, Loss: 0.7346, Acc: 0.7083\n",
            "Epoch: 25/100, Loss: 0.7245, Acc: 0.7167\n",
            "Epoch: 26/100, Loss: 0.6855, Acc: 0.7167\n",
            "Epoch: 27/100, Loss: 0.6721, Acc: 0.7167\n",
            "Epoch: 28/100, Loss: 0.5991, Acc: 0.7083\n",
            "Epoch: 29/100, Loss: 0.6999, Acc: 0.7167\n",
            "Epoch: 30/100, Loss: 0.8637, Acc: 0.7333\n",
            "Epoch: 31/100, Loss: 0.6545, Acc: 0.7417\n",
            "Epoch: 32/100, Loss: 0.5701, Acc: 0.7750\n",
            "Epoch: 33/100, Loss: 0.5964, Acc: 0.7833\n",
            "Epoch: 34/100, Loss: 0.5554, Acc: 0.7917\n",
            "Epoch: 35/100, Loss: 0.5097, Acc: 0.8083\n",
            "Epoch: 36/100, Loss: 0.5005, Acc: 0.8250\n",
            "Epoch: 37/100, Loss: 0.4994, Acc: 0.8333\n",
            "Epoch: 38/100, Loss: 0.6038, Acc: 0.8500\n",
            "Epoch: 39/100, Loss: 0.4755, Acc: 0.8500\n",
            "Epoch: 40/100, Loss: 0.5336, Acc: 0.8583\n",
            "Epoch: 41/100, Loss: 0.5976, Acc: 0.8750\n",
            "Epoch: 42/100, Loss: 0.3653, Acc: 0.8917\n",
            "Epoch: 43/100, Loss: 0.3801, Acc: 0.9000\n",
            "Epoch: 44/100, Loss: 0.3777, Acc: 0.9000\n",
            "Epoch: 45/100, Loss: 0.3075, Acc: 0.9167\n",
            "Epoch: 46/100, Loss: 0.2720, Acc: 0.9167\n",
            "Epoch: 47/100, Loss: 0.2846, Acc: 0.9167\n",
            "Epoch: 48/100, Loss: 0.3780, Acc: 0.9167\n",
            "Epoch: 49/100, Loss: 0.3200, Acc: 0.9167\n",
            "Epoch: 50/100, Loss: 0.3212, Acc: 0.9250\n",
            "Epoch: 51/100, Loss: 0.3627, Acc: 0.9250\n",
            "Epoch: 52/100, Loss: 0.4161, Acc: 0.9250\n",
            "Epoch: 53/100, Loss: 0.3433, Acc: 0.9250\n",
            "Epoch: 54/100, Loss: 0.2328, Acc: 0.9250\n",
            "Epoch: 55/100, Loss: 0.2201, Acc: 0.9250\n",
            "Epoch: 56/100, Loss: 0.3826, Acc: 0.9250\n",
            "Epoch: 57/100, Loss: 0.1960, Acc: 0.9333\n",
            "Epoch: 58/100, Loss: 0.2661, Acc: 0.9333\n",
            "Epoch: 59/100, Loss: 0.2578, Acc: 0.9333\n",
            "Epoch: 60/100, Loss: 0.1740, Acc: 0.9333\n",
            "Epoch: 61/100, Loss: 0.2996, Acc: 0.9333\n",
            "Epoch: 62/100, Loss: 0.2899, Acc: 0.9333\n",
            "Epoch: 63/100, Loss: 0.2234, Acc: 0.9333\n",
            "Epoch: 64/100, Loss: 0.2255, Acc: 0.9333\n",
            "Epoch: 65/100, Loss: 0.2446, Acc: 0.9417\n",
            "Epoch: 66/100, Loss: 0.2467, Acc: 0.9500\n",
            "Epoch: 67/100, Loss: 0.1619, Acc: 0.9500\n",
            "Epoch: 68/100, Loss: 0.1781, Acc: 0.9500\n",
            "Epoch: 69/100, Loss: 0.1342, Acc: 0.9500\n",
            "Epoch: 70/100, Loss: 0.1900, Acc: 0.9500\n",
            "Epoch: 71/100, Loss: 0.2040, Acc: 0.9500\n",
            "Epoch: 72/100, Loss: 0.1318, Acc: 0.9500\n",
            "Epoch: 73/100, Loss: 0.2130, Acc: 0.9500\n",
            "Epoch: 74/100, Loss: 0.1569, Acc: 0.9500\n",
            "Epoch: 75/100, Loss: 0.2004, Acc: 0.9500\n",
            "Epoch: 76/100, Loss: 0.1169, Acc: 0.9500\n",
            "Epoch: 77/100, Loss: 0.1844, Acc: 0.9667\n",
            "Epoch: 78/100, Loss: 0.1216, Acc: 0.9667\n",
            "Epoch: 79/100, Loss: 0.0932, Acc: 0.9667\n",
            "Epoch: 80/100, Loss: 0.0964, Acc: 0.9667\n",
            "Epoch: 81/100, Loss: 0.1187, Acc: 0.9667\n",
            "Epoch: 82/100, Loss: 0.1645, Acc: 0.9667\n",
            "Epoch: 83/100, Loss: 0.1330, Acc: 0.9667\n",
            "Epoch: 84/100, Loss: 0.1499, Acc: 0.9667\n",
            "Epoch: 85/100, Loss: 0.1125, Acc: 0.9667\n",
            "Epoch: 86/100, Loss: 0.1186, Acc: 0.9583\n",
            "Epoch: 87/100, Loss: 0.1530, Acc: 0.9583\n",
            "Epoch: 88/100, Loss: 0.0971, Acc: 0.9583\n",
            "Epoch: 89/100, Loss: 0.1566, Acc: 0.9583\n",
            "Epoch: 90/100, Loss: 0.0927, Acc: 0.9583\n",
            "Epoch: 91/100, Loss: 0.1271, Acc: 0.9583\n",
            "Epoch: 92/100, Loss: 0.1800, Acc: 0.9583\n",
            "Epoch: 93/100, Loss: 0.0723, Acc: 0.9667\n",
            "Epoch: 94/100, Loss: 0.0749, Acc: 0.9667\n",
            "Epoch: 95/100, Loss: 0.1063, Acc: 0.9667\n",
            "Epoch: 96/100, Loss: 0.0888, Acc: 0.9667\n",
            "Epoch: 97/100, Loss: 0.0810, Acc: 0.9667\n",
            "Epoch: 98/100, Loss: 0.0892, Acc: 0.9667\n",
            "Epoch: 99/100, Loss: 0.0673, Acc: 0.9750\n",
            "Epoch: 100/100, Loss: 0.0602, Acc: 0.9750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test evaluation"
      ],
      "metadata": {
        "id": "uKZjHGz79Sf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # weights cannot be modified(frozen)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad(): # close the gradient calculation mechanism\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    Y_pred = model(X_batch)\n",
        "    _, predicted = torch.max(Y_pred.data, 1)\n",
        "    total += Y_batch.size(0)\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "    y_true.extend(Y_batch.squeeze().cpu().numpy())\n",
        "    y_pred.extend(predicted.cpu().numpy())\n",
        "  print(f\"Test Acc: {correct / total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdP1lCSE9Ulm",
        "outputId": "d33f6b50-f285-4f2c-d039-a0ba6704aa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer prediction"
      ],
      "metadata": {
        "id": "vG6jMUGMCvKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({\n",
        "    \"Y_true\": y_true,\n",
        "    \"Y_pred\": y_pred\n",
        "})\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkBM6ezsCNwY",
        "outputId": "50ed7df5-ed8c-4eb0-8310-253ac3646670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Y_true  Y_pred\n",
            "0        2       2\n",
            "1        1       1\n",
            "2        0       0\n",
            "3        2       2\n",
            "4        0       0\n",
            "5        2       2\n",
            "6        0       0\n",
            "7        1       1\n",
            "8        1       1\n",
            "9        1       1\n",
            "10       2       2\n",
            "11       1       1\n",
            "12       1       1\n",
            "13       1       1\n",
            "14       1       1\n",
            "15       0       0\n",
            "16       1       1\n",
            "17       1       1\n",
            "18       0       0\n",
            "19       0       0\n",
            "20       2       2\n",
            "21       1       1\n",
            "22       0       0\n",
            "23       0       0\n",
            "24       2       2\n",
            "25       0       0\n",
            "26       0       0\n",
            "27       1       1\n",
            "28       1       1\n",
            "29       0       0\n"
          ]
        }
      ]
    }
  ]
}