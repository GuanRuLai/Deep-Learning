{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwc4aBvPuQK4dPMO3miv0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuanRuLai/Python-Deep-Learning/blob/main/Pytorch_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "U5G6W9Ta5xrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "-othZhNn7Ppy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9zFEx-53Xc_",
        "outputId": "a3981fc5-f86e-4e63-a52f-033a0e139daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['北师教育学，你我一起努力，让胜利酣畅淋漓。', '考博英语词汇', '出售人大新闻学院2015年考研权威资料', '【脑科院 郭桃梅课题组】科研助理招聘', '管理学院的同学帮帮忙呐～']\n",
            "['【字节跳动内推】校招岗位全面开放，帮查进度！', '招聘兼职/ 笔试考务 /200-300 每人', '国企出版社招聘坐班兼职生', '【在线早教】教研实习生招聘', '【兼职】心理学公众号寻兼职写手']\n"
          ]
        }
      ],
      "source": [
        "academy_titles = []\n",
        "job_titles = []\n",
        "\n",
        "with open(\"academy_titles.txt\", \"r\", encoding= \"utf-8\") as f:\n",
        "    for l in f:\n",
        "      academy_titles.append(l.strip()) # remove spaces of head and tail\n",
        "\n",
        "with open(\"job_titles.txt\", \"r\", encoding= \"utf-8\") as f:\n",
        "    for l in f:\n",
        "      job_titles.append(l.strip()) # remove spaces of head and tail\n",
        "\n",
        "print(academy_titles[:5])\n",
        "print(job_titles[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word tokenizing"
      ],
      "metadata": {
        "id": "oQfokLiJ_3Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = set()\n",
        "\n",
        "for title in academy_titles:\n",
        "    for char in title:\n",
        "        char_set.add(char)\n",
        "\n",
        "for title in job_titles:\n",
        "    for char in title:\n",
        "        char_set.add(char)\n",
        "\n",
        "print(char_set)\n",
        "print(len(char_set))\n",
        "\n",
        "char_list = list(char_set) # typecasting: set to list\n",
        "n_chars = len(char_list) + 1 # +1 for non-existent characters(<unk>)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxQm6ey19Yv6",
        "outputId": "221fa143-2282-4370-9923-c4cd7cf34308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'建', '青', '行', '在', '法', '资', '涛', '皮', '裁', '宾', '虚', '督', '增', '妈', '披', '趣', '原', '打', '翱', '企', '小', '熙', '细', '嘉', '旁', '龟', '已', '后', '们', 'k', '式', '食', '府', '哲', 'U', '筛', '断', '燃', '段', '祖', '律', '太', '益', '欢', '运', '专', '世', '球', '七', '滑', '哦', '条', '树', '申', '档', '嘎', '技', '剧', '沉', '叒', '鑫', '绘', '孚', '队', '得', '报', '威', '料', '将', '东', '徒', '逸', 'z', '●', '*', '什', '柳', '介', '态', '恩', '佳', '账', '馆', '花', '咕', '斩', '核', '住', 'W', '说', '忙', '圈', '农', 'm', '选', '去', '科', '是', '跨', '匠', '汇', '朋', '协', '战', 'V', '淀', '!', '政', '象', '航', '端', '篇', '朗', '右', '闻', '未', '加', '暇', '驾', '义', '央', '～', '障', '一', '页', 'B', '充', '旷', '冠', '滴', '矩', '也', '苏', '塾', '审', '季', '渠', '️', '芯', '望', '造', '沟', '承', '看', '高', '闲', '史', '顾', '或', '阅', '班', '极', '径', '启', '开', '边', '究', '箱', '变', '设', '招', '宏', '字', '立', '棋', '装', '尚', '缘', '千', '总', '帝', '署', '男', '试', '办', '浙', '改', '面', '举', '霸', 's', '尽', '宿', '托', '年', '>', '妆', '豆', '见', '大', '驶', '剑', '蒙', '艾', '！', '藤', '织', '锁', '精', '武', '克', '复', '芋', '择', '功', '废', '锦', '猬', '.', '护', '访', '京', '简', '潮', '器', '‘', '湿', '哪', '摩', '旗', '脑', '®', '迪', '辅', '趁', '理', '征', '健', '马', '妹', '袖', '生', '二', '会', '君', '熟', '引', '谢', '奖', '凌', '卡', '委', '院', '锋', '三', '舟', '己', '万', '冀', '北', '游', '规', '靠', '侨', '斯', '@', '韬', '韩', '吾', '乳', '九', '新', '可', '帮', '坝', '款', '寻', '旋', '素', '单', '』', '泸', '全', 'r', '具', '午', '之', '届', '盆', '牌', '仕', '等', '气', '拿', '临', '陶', '术', '聚', '继', '撼', '攀', '诈', '兼', '师', '渴', '我', '执', 'p', '祥', '防', '助', '制', '固', '『', '投', '悦', '事', '怠', '筑', '腾', 'y', '驻', '策', '交', '错', '%', '采', '坡', '言', '副', '函', '畜', '巴', '孔', '材', 'd', '境', '育', '辉', '寒', '娃', '暂', '那', '波', '么', '第', '辽', '座', '河', '枝', '猫', '剪', '休', '历', '台', '意', '约', '容', '尘', '就', '暨', '虾', '觉', '息', '离', '始', '鱼', '浦', '腿', '歌', '著', '野', '留', '还', '了', '氛', '因', '顶', '流', '求', '雨', '齿', '处', '林', '鹤', '咨', '毕', '甘', '旭', '“', '赐', '比', '冰', '递', '集', '煌', '坪', '坐', '赁', '教', '纽', 'P', '│', '猪', '怀', '亮', '进', '偶', '5', '贝', '满', '川', '快', '琴', '贴', '置', '租', '恤', '淄', '｜', '幼', '期', '险', '。', '王', '索', '俭', '疑', '康', '评', '末', '池', '入', '戳', '用', '故', '贫', '众', '3', '+', '呐', '骐', '鲜', '而', '寿', '飞', '沈', '记', '略', '询', '董', '郭', '菱', '动', '培', '葱', '邀', '挑', '几', '余', '陪', '又', '刑', '･', '的', '源', '跑', '庭', '遥', 'H', '送', '零', '够', '雍', '紧', '\"', '析', '想', '路', '百', '丽', '免', '美', '怡', '壳', '绍', '问', '嗅', '虫', '希', '准', '笃', '金', '配', '净', '-', '每', '构', '目', 't', '凰', '光', '格', '只', '钟', '伙', '艳', 'j', '挖', '聿', '乌', '国', '澳', '屋', '殊', '塑', '褥', 'E', '春', '辜', '着', '预', '田', '卷', '帆', '葡', '淋', '罗', '畅', '跪', '家', '型', '插', '倒', '谷', '渝', '赣', '量', '啦', '；', '曼', '店', '(', '烟', '过', '维', '香', '起', '＃', '接', '束', '募', '祝', '励', '揣', '检', '分', ':', '区', '揽', '洽', '情', '证', '板', 'l', '凝', '纯', '厅', '常', '（', '和', '词', '商', '局', '激', '岭', '消', '足', '结', '伟', '帖', '靖', '疫', '︱', '医', '与', '链', '整', '老', '蓝', '舆', '官', '含', '岁', '吉', '搞', '酵', '备', '宁', '兆', '便', '戏', '止', '《', '博', '员', 'b', '必', '费', '］', '效', '遇', '陕', '仿', '截', '早', '旅', '终', '带', '于', '样', '浩', '尼', '议', '涯', '对', '告', '[', '翼', '畏', '军', '坊', '习', '角', '谁', '能', '识', '〜', '街', '杉', '珠', '例', '话', '盛', '孩', '鲲', '仁', '晶', '玉', '节', '散', '刊', 'C', '算', '明', '蝠', '森', '刻', '些', '&', '月', '股', '卫', '除', '围', '陷', '址', '斑', '步', '漓', '盖', '雅', '融', '：', '困', '失', '耐', '际', '童', '贷', '阿', '宇', '港', '很', '避', '~', '综', '张', '红', '然', '粮', '触', '肃', '粉', '佛', '兽', '浸', '´', '身', '水', '竞', '估', '秀', '叮', '庄', '鹅', '题', '蔚', '观', '|', '赚', '搬', '鹏', '才', '倦', '块', '盟', '州', '印', '萄', '蛋', '广', '蛟', '糖', '当', '刺', '排', '眼', '艺', '犀', '龙', '此', '待', '输', '杯', '考', '聘', '欺', '冶', '别', '纪', '市', '品', '作', '片', '反', '温', '普', '英', '完', '验', '即', '势', '以', '」', '受', '割', '默', '劼', '鲁', '塘', '翻', '治', '统', '诸', '诚', '需', '写', '创', '走', '版', '汤', '扩', '喜', '若', '？', '础', '外', '曲', '晚', '卖', 'a', '爱', '榛', '买', '磅', 'F', '贺', '施', ')', '赛', '石', '、', '视', '席', '要', '矿', '智', 'f', '密', '谊', '权', '芽', '难', '列', '溪', '谈', '沙', '暑', '物', '镇', '闭', '签', '津', '服', '删', '另', '村', '逐', '豪', 'c', '秋', '肆', '鞋', '诗', '<', '业', '网', '沃', '络', '宝', '再', '程', '关', '力', '厚', '齐', '婚', '价', '封', '同', '特', '出', '尖', '成', '下', '洁', '学', '2', '衣', '不', '底', '佰', '今', '册', '参', '_', '哈', '长', '书', '更', '主', '这', '久', '园', '越', '四', '丨', '工', '首', '楼', '钉', '曙', '9', '属', '岳', '肚', '重', '0', '室', '序', '无', '坛', '漫', 'O', '菲', '地', '兴', '为', 'L', '「', '向', '拍', '》', '安', '润', '撰', '养', '发', '星', '神', '搜', '斗', '实', '堂', '杰', '读', '火', '追', '绿', '蓉', '瑞', '漂', '亦', '宜', '支', '咯', '梳', '渤', '饮', '定', '济', '吃', '禾', '续', '梯', '基', '华', '个', '钱', '判', '苍', '中', '级', '…', '本', '供', '良', '案', '积', '近', '娱', '{', '柏', '荐', '海', '都', '叕', '硬', '榜', '者', '嘴', '延', '棒', '项', '模', '六', '较', '呼', '指', '课', '补', '族', '盈', '衔', '半', '超', '慧', '讨', '团', '庙', '毅', '你', '相', '促', '米', '晖', '黄', '船', '系', '女', '脉', '仪', '锅', '您', '俱', '鲸', '亚', '手', '空', '先', '份', '碰', '爬', '乘', '辆', '幕', '放', '澜', 'M', '恢', '淘', '凯', '士', '茗', '某', '·', '铝', '奥', '责', '昆', '振', '拟', '`', '初', '号', '论', '赴', '破', '朱', '杭', '宣', '绩', '碎', '宠', '解', '揭', '述', '八', '银', '轮', '联', 'n', '里', '让', '餐', '提', '组', 'G', '咪', '其', '瑅', '存', '批', '正', '性', '梅', '给', '汽', '禹', '人', '找', '画', '跳', '贪', '职', '志', '骄', '笔', '察', '适', 'K', '低', '福', '纵', '左', '表', '翔', '自', '翰', '宫', '门', '摄', '享', '录', '渡', '度', '乎', '–', '尔', '1', '牧', 'N', '练', '枚', '换', '桃', '称', '升', '训', '玛', '湖', '泰', '迈', '昱', '蔬', '蚂', '各', '贵', '私', '亲', '做', '答', '居', 'e', '颇', '鲤', '践', '皆', '玩', 'v', '礼', '非', '崂', '6', '岛', '庆', '牛', '械', '币', '纬', '橙', '限', '展', '距', '声', '踪', '迎', '类', '干', '吧', '由', '部', '丰', '研', '多', '洲', '销', '讯', '邮', '露', '陆', '豚', '转', 'Y', '拓', '滨', '}', '社', '房', '随', '悉', '雪', '微', '莱', '图', '袭', '爆', '所', '包', '守', 'R', '讲', '梦', '位', '污', '猿', '震', '达', '璞', '奇', '盒', '站', '闪', '遵', '衡', '雷', '猴', '召', '德', '勤', '乐', '播', '管', '舶', '优', '编', '架', '巨', '8', '查', '回', '脚', 'X', '频', '登', ' ', '凤', '好', '界', '调', '纳', '影', '狗', '往', '付', '扫', '南', '哥', '他', '少', '虎', '昌', '4', '愿', '通', '慕', '劳', '惠', '阳', '衍', '茂', '口', '前', '券', '挂', '谱', '—', '音', '7', '最', '税', '佬', '云', '监', '壹', '诊', '注', '景', '滇', '蝙', '群', '－', '抖', '姐', '合', '¥', '芃', '把', '归', '扶', '响', '元', '瓜', '货', '载', '伴', '探', '致', '库', '直', '汕', '推', '冬', '彩', '李', '伦', '互', '间', '心', '山', '岸', '风', '译', '阶', '江', 'I', '骗', '民', '贸', '电', '演', '儿', '次', '灾', '胜', '镜', '猎', '远', '土', '认', '活', '蚁', '杂', '机', '附', '质', '被', '软', '籍', '氪', '汉', '圣', '帜', '啊', '上', 'o', '道', '公', '友', '垂', '锐', '”', '授', '有', '白', '域', '母', 'D', '胡', '日', '修', '层', 'ᴗ', '胞', 'i', '牙', 'Q', '丘', '鸟', '睿', '停', '党', '请', '没', '浪', '永', '产', '校', '省', '邦', ',', '订', '子', '导', '独', '周', '收', '松', '五', '经', '领', '及', '厦', '色', '令', '厂', '洋', 'S', '真', '仔', '丹', '范', '哒', '语', '平', '崔', '似', '感', '来', '占', '保', '体', '速', '努', '任', '控', '圳', '哟', '覆', '背', '晓', '营', '铁', '蓄', '杜', '思', '羽', '草', 'u', '^', '假', '欧', '命', '岗', 'T', 'h', '填', '姆', '乡', '硕', '菜', '获', '偏', '兵', '码', '弟', '兄', '额', '晨', '】', '果', '贤', '测', '撤', '跟', '听', '利', '忆', '缺', '辑', '富', '均', '崇', '/', '司', '热', '耀', '灵', '扑', 'w', '仑', '时', '点', '持', ']', '示', '清', '骥', '西', '／', '户', '媒', '木', '桂', '滤', '峰', '掘', '油', '陌', '迦', '懂', '深', '争', '唐', '偿', '县', '［', '划', '代', '购', '两', '秘', '铭', '强', '酣', '吗', '计', '到', '荷', 'x', '蜂', '短', '雄', '’', '线', '稿', '陈', '夫', '览', '标', '双', '桥', '负', '献', '，', '呀', 'g', '像', '咚', '文', '头', '差', '恒', '传', '秒', '方', '榆', '储', '塔', '从', '务', '种', '畔', '客', '据', '#', '荟', '现', '顿', '章', '杨', '场', '幸', '药', 'A', '债', '牡', '辰', '酬', '伊', '决', '照', '叔', '洱', '环', '薪', '内', '菁', '应', '搭', '舞', '使', '易', '件', '移', '卓', '【', 'J', '城', '窖', '知', '轻', '朴', '炎', '朝', '急', '匹', '横', '数', '售', '车', '\\\\', '峡', '★', '共', '钢', '绽', '碧', '莞', '疆', '夏', '掌', '毛', '呦', '倍', '按', '）', '繁', '蔓', '洗', '财', '名', '化', '信', '天', '十', '古', '甲', '凡'}\n",
            "1570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert title strinsg to a tensor of character indices"
      ],
      "metadata": {
        "id": "auotF0D2Bxb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def title_to_tensor(title):\n",
        "    tensor = torch.zeros(len(title), dtype=torch.long) # initialize a tensor of zeros with the length of the title\n",
        "\n",
        "    for li, char in enumerate(title):\n",
        "      try:\n",
        "        ind = char_list.index(char) + 1 # +1 to differentiate \"0\" in index and \"0\" in zero tensor\n",
        "      except ValueError:\n",
        "        ind = n_chars - 1 # -1 to ensure the highest index for unknown characters are reserved\n",
        "      tensor[li] = ind\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "8HLgVAnpAF7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build dataset & Split independent variables and dependent variable"
      ],
      "metadata": {
        "id": "Grup99XwVpFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = []\n",
        "\n",
        "for l in academy_titles:\n",
        "    all_data.append((title_to_tensor(l), torch.tensor([0], dtype=torch.long)))\n",
        "\n",
        "for l in job_titles:\n",
        "    all_data.append((title_to_tensor(l), torch.tensor([1], dtype=torch.long)))\n",
        "\n",
        "print(all_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZT3t0dZVuup",
        "outputId": "90b0bcc6-834a-4338-dd65-b5142a97de3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor([ 250,  294,  390,  328,  838, 1476,  963,  296,  124,  538, 1365,  825,\n",
            "        1476, 1025, 1276, 1408, 1456,  519,  517,  660,  414]), tensor([0])), (tensor([ 726,  595,  739, 1355,  562,  101]), tensor([0])), (tensor([ 833, 1541, 1041,  190,  262,  115,  838,  243,  839,  869, 1071,  399,\n",
            "         185,  726, 1125,  792,   67,    6,   68]), tensor([0])), (tensor([1528,  221,   97,  243, 1187,  442, 1077, 1037,  951,  697, 1028, 1401,\n",
            "          97, 1125,  301,  226,  161,  727]), tensor([0])), (tensor([1174,  226,  838,  243,  455,  831,  838,  264,  264,   91,  431,  122]), tensor([0]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split training set and testing set"
      ],
      "metadata": {
        "id": "Zk4WiSqcZOPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm1MoS1fZPNH",
        "outputId": "49fe8470-b9db-4035-ce27-a2a3eccf6199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5686\n",
            "1422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define collate function to handle variable-length sequences"
      ],
      "metadata": {
        "id": "XKEcalvMw3Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    titles, labels = zip(*batch)\n",
        "    titles_padded = pad_sequence(titles, batch_first=True, padding_value=0) # make all sequences the same length\n",
        "    labels = torch.cat(labels) # concatenate labels\n",
        "    return titles_padded, labels"
      ],
      "metadata": {
        "id": "bjOuI3FMw686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network processing"
      ],
      "metadata": {
        "id": "vgU0PiPKH9Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if there is GPU to use"
      ],
      "metadata": {
        "id": "rNhvchaHNLmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU available, using CPU instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji7GQlpbNNMK",
        "outputId": "4a1a846e-2936-47ce-ea02-89cfb8763522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "NkV8JMoAH-7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "\n",
        "class RnnModel(nn.Module):\n",
        "\n",
        "    # define frameworks of each neural layer\n",
        "    def __init__(self, word_count, embedding_size, hidden_size, output_size):\n",
        "        super(RnnModel, self).__init__()\n",
        "\n",
        "        # define neural layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(word_count, embedding_size) # embedding layer\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers=1, bidirectional=False, batch_first=True) # RNN layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size) # full connected layer\n",
        "\n",
        "        # define weight initializers of each layer(default)\n",
        "        init.xavier_normal_(self.fc.weight)\n",
        "\n",
        "    # define forward propagation function to connect layers(including activation function)\n",
        "    def forward(self, input_tensor):\n",
        "      word_vector = self.embedding(input_tensor)\n",
        "      output, (hidden, _) = self.rnn(word_vector) # LSTM returns output: (hidden, cell), we need hidden state of the last time step.\n",
        "      output = self.fc(hidden[-1]) # extract the hidden state of the last layer\n",
        "      return output\n",
        "\n",
        "# set hyperparameter values\n",
        "word_count = n_chars\n",
        "embedding_size = 200\n",
        "hidden_size = 10\n",
        "output_size = 2\n",
        "\n",
        "# initialize model\n",
        "model = RnnModel(word_count, embedding_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "3agEZNwEIAiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training & evaluation"
      ],
      "metadata": {
        "id": "9mqQtTuPau52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "rPBN7QcAcfOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  model.train() # weights can be modified\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for X_batch, Y_batch in train_loader:\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "    optimizer.zero_grad() # return zero of every previous batch\n",
        "\n",
        "    Y_pred = model(X_batch)\n",
        "    loss = criterion(Y_pred, Y_batch.squeeze())\n",
        "    loss.backward() # calculate gradient(min loss weights)\n",
        "    optimizer.step() # update weights\n",
        "\n",
        "    _, predicted = torch.max(Y_pred.data, 1) # get the index of max value in each row of axis 1\n",
        "    total += Y_batch.size(0) # get the number of samples\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "\n",
        "  accuracy = correct / total\n",
        "  print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Acc: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-YQdIqXyAhf",
        "outputId": "6d6c98da-1258-4168-b702-a681b40cb14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5, Loss: 0.0073, Acc: 0.9559\n",
            "Epoch: 2/5, Loss: 0.0003, Acc: 0.9989\n",
            "Epoch: 3/5, Loss: 0.0002, Acc: 0.9995\n",
            "Epoch: 4/5, Loss: 0.0001, Acc: 0.9996\n",
            "Epoch: 5/5, Loss: 0.0001, Acc: 0.9995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test evaluation"
      ],
      "metadata": {
        "id": "58EgChOa0Miy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # weights cannot be modified(frozen)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad(): # close the gradient calculation mechanism\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for X_batch, Y_batch in test_loader:\n",
        "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "    Y_pred = model(X_batch)\n",
        "    _, predicted = torch.max(Y_pred.data, 1)\n",
        "    total += Y_batch.size(0)\n",
        "    correct += (predicted == Y_batch.squeeze()).sum().item()\n",
        "    y_true.extend(Y_batch.squeeze().cpu().numpy())\n",
        "    y_pred.extend(predicted.cpu().numpy())\n",
        "  print(f\"Test Acc: {correct / total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYKN4tPV0NgH",
        "outputId": "c1336d13-8658-45aa-df49-3f2999f95433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer prediction"
      ],
      "metadata": {
        "id": "qCf3lRbE0WQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"Y_true\": y_true,\n",
        "    \"Y_pred\": y_pred\n",
        "})\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crXbcg_c0YBJ",
        "outputId": "51e69341-e66b-4fab-cced-cb72753984f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Y_true  Y_pred\n",
            "0          1       1\n",
            "1          1       1\n",
            "2          1       1\n",
            "3          1       1\n",
            "4          1       1\n",
            "...      ...     ...\n",
            "1417       1       1\n",
            "1418       0       0\n",
            "1419       1       1\n",
            "1420       0       0\n",
            "1421       0       0\n",
            "\n",
            "[1422 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}