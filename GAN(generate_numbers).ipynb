{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+0PZUYcgc1j1yQ4wj3N23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuanRuLai/Python-Deep-Learning/blob/main/GAN(generate_numbers).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "rcwYNzAxJTL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset & Split training and testing set"
      ],
      "metadata": {
        "id": "pskVLXOcJUxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pQDJNgsfWxp",
        "outputId": "1393728d-d53b-4a94-affe-b8688fa0704b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
        "from keras.models import Sequential\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, _), (_, _) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rescale -1 to 1 to fit tanh() activation function of output layer"
      ],
      "metadata": {
        "id": "i6j9lRYIKaoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5"
      ],
      "metadata": {
        "id": "tbvcAcKrKkAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add a color channel dimension"
      ],
      "metadata": {
        "id": "aslwjFqeKrp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train =  np.expand_dims(X_train, axis=3)"
      ],
      "metadata": {
        "id": "tuCTKQvsKvCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network processing"
      ],
      "metadata": {
        "id": "HQgFoaMULPs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the hyperparameters"
      ],
      "metadata": {
        "id": "1v_6AmLtK1cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the shape of MNIST\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "# set the length of noise for GANs generator input layer\n",
        "z_dim = 100\n",
        "\n",
        "epochs = 10000\n",
        "batch_size = 128\n",
        "\n",
        "# set the times of training that shows acc / loss once\n",
        "sample_interval = 100"
      ],
      "metadata": {
        "id": "JeUA4S9NrX1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define generator"
      ],
      "metadata": {
        "id": "3T0NGKZFL7-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(img_shape, z_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=z_dim))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "    model.add(Reshape(img_shape))\n",
        "    return model"
      ],
      "metadata": {
        "id": "gtvDvKzz39nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define discriminator"
      ],
      "metadata": {
        "id": "x7YuhW4SMZ9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "vBjJHLCp4Ljj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GAN: combine generator and discriminator"
      ],
      "metadata": {
        "id": "QzFxi7uUMm7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model"
      ],
      "metadata": {
        "id": "n8GC-64K4Q9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and compile discriminator"
      ],
      "metadata": {
        "id": "zDhrTTSWM1cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vrNa5Dx59tN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze discriminator's weights when training generator"
      ],
      "metadata": {
        "id": "hCkeOoOdN2sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator.trainable = False"
      ],
      "metadata": {
        "id": "MGvFWc7T-JK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create generator"
      ],
      "metadata": {
        "id": "U_LI5aKRNKBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator(img_shape, z_dim)"
      ],
      "metadata": {
        "id": "xeTO4mqb939m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and compile GAN"
      ],
      "metadata": {
        "id": "_gL-pOf0N99L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer=\"adam\", loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "fvy1Q_C--l3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the function of showing graphs during training process"
      ],
      "metadata": {
        "id": "PX_-Bah2OX4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_images(generator, image_grid_rows=4, image_grid_columns=4):\n",
        "    # sample random noise\n",
        "    noise = np.random.normal(loc=0, scale=1, size=(image_grid_rows * image_grid_columns, z_dim))\n",
        "\n",
        "    # generate images from random noise\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # rescale image pixel values to [0, 1]\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    # Set image grid\n",
        "    fig, axs = plt.subplots(image_grid_rows,\n",
        "                            image_grid_columns,\n",
        "                            figsize=(4, 4),\n",
        "                            sharey=True,\n",
        "                            sharex=True)\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            # Output a grid of images\n",
        "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Bq44M7ze_bgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training function"
      ],
      "metadata": {
        "id": "hFEgrB_EOmSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "    # labels for real images: all ones\n",
        "    real = np.ones(batch_size, 1) # 128 rows, 1 column\n",
        "\n",
        "    # labels for fake images: all zeros\n",
        "    fake = np.zeros(batch_size, 1) # 128 rows, 1 column\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # turn off the stdout temporarily\n",
        "        # to supress 4/4 [=====================] - 0s 3ms/step\n",
        "        original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "        # ---- train discriminator ----\n",
        "\n",
        "        # get a random batch(128) of real images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size) # get 128 columns from index 0~59999\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # generate a batch of fake images\n",
        "        noise = np.random.normal(0, 1, (batch_size, z_dim)) # mean=0, std=1, 128 rows, 100 columns\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # train discriminator\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---- train generator ----\n",
        "\n",
        "        # generate a batch of fake images\n",
        "        noise = np.random.normal(0, 1, (batch_size, z_dim)) # mean=0, std=1, 128 rows, 100 columns\n",
        "\n",
        "        # set the noise as True(1) in purpose\n",
        "        g_loss = gan.train_on_batch(noise, real)\n",
        "\n",
        "        # turn stdout back on\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = original_stdout\n",
        "\n",
        "        if (iteration + 1) % sample_interval == 0:\n",
        "\n",
        "            # save losses and accuracies so they can be plotted after training\n",
        "            losses.append((d_loss, g_loss))\n",
        "            accuracies.append(100.0 * accuracy)\n",
        "            iteration_checkpoints.append(iteration + 1)\n",
        "\n",
        "            # output training progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n",
        "                (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n",
        "\n",
        "            # output a sample of generated image\n",
        "            sample_images(generator)"
      ],
      "metadata": {
        "id": "A-UOseArB6_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "ggUHbqCPOw79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(iterations=epochs, batch_size=batch_size, sample_interval=sample_interval)"
      ],
      "metadata": {
        "id": "QBdzSu-aYgyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}